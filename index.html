<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/jumbo.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>VoxelKP</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .bgImg {
      position: relative;
      width: 100%;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .bgImg::before {    
      content: "";
      background-image: url('./static/images/jumbo.png');
      background-size: cover;
      position: absolute;
      top: 0px;
      right: 0px;
      bottom: 0px;
      left: 0px;
      opacity: 0.2;
    }
    .item img {
      width: 100%;
      height: auto;
      object-fit: cover;
    }

    .video-container {
      position: relative;
      padding-bottom: 56.25%; /* 16:9 */
      height: 0;
    }
    .video-container iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }
  </style>
</head>
<body>
  <section class="hero bgImg">
    <div class="hero-body header-contents">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VoxelKP: A Voxel-based Network Architecture for Human Keypoint Estimation in LiDAR Data</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Jian Shi</a>,</span>
                <span class="author-block">
                  <a target="_blank">Peter Wonka</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">King Abdullah University of Science and Technology<br><!-- Conferance name and year --></span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="./static/pdfs/Shi_LiDAR_Keypoints.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/shijianjian/VoxelKP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.08871" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="video-container">
      <iframe src="https://www.youtube.com/embed/u-xHv_OAO0M">
      </iframe>
    </div>
  </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <i>VoxelKP</i>, a novel fully sparse network architecture tailored for human keypoint estimation in LiDAR data. The key challenge is that objects are distributed sparsely in 3D space, while human keypoint detection requires detailed local information wherever humans are present. We propose four novel ideas in this paper. First, we propose sparse selective kernels to capture multi-scale context. Second, we introduce sparse box-attention to focus on learning spatial correlations between keypoints within each human instance. Third, we incorporate a spatial encoding to leverage absolute 3D coordinates when projecting 3D voxels to a 2D grid encoding a bird's eye view. Finally, we propose hybrid feature learning to combine the processing of per-voxel features with sparse convolution. We evaluate our method on the Waymo dataset and achieve an improvement of 27% on the MPJPE metric compared to the state-of-the-art, <i>HUM3DIL</i>, trained on the same data, and 12% against the state-of-the-art, <i>GC-KPL</i>, pretrained on a 25X larger dataset. To the best of our knowledge, <i>VoxelKP</i> is the first single-staged, fully sparse network that is specifically designed for addressing the challenging task of 3D keypoint estimation from LiDAR data, achieving state-of-the-art performances.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="./static/images/jumbotron.drawio.png" alt="">
        <p class="subtitle has-text-centered">
          A visual demonstration of our baseline model (top) and the proposed VoxelKP (bottom). Our VoxelKP offers improved keypoint estimation with precise locations and fewer false positives. The insets are color-coded according to the legend in the figure. In the green-colored insets, a comparison with the ground truth is shown, with ground truth in red and predictions in blue.
        </p>
      </div>
      <div class="item">
        <img src="./static/images/jump-supp1.drawio.png" alt="">
        <h2 class="subtitle has-text-centered">
          A visual demonstration of our baseline model (left) and the proposed VoxelKP (right). Our method produces fewer false positives.
        </h2>
      </div>
      <div class="item">
        <img style="width: 100%;" src="./static/images/jump-supp2.drawio-1.png" alt="">
        <img style="width: 100%;" src="./static/images/jump-supp2.drawio-2.png" alt="">
        <h2 class="subtitle has-text-centered">
          Baseline model (top) and VoxelKP (bottom). Our method detects the human objects that the baseline method fails to.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">The Network Architecture</h2>
      <div class="content has-text-justified">
        <p style="padding-bottom: 1em;">LiDAR point clouds typically contain sparsely distributed objects that occupy only small regions of the full 3D space. While the distribution of humans in space is sparse, in contrast, human keypoints require dense information wherever a human is present. To handle this density variation, we aim to improve feature learning in the regions where keypoints need to be located and detailed information is required.</p>
        <img style="margin-left: auto;margin-right: auto;display: block;" src="./static/images/arch2.drawio.png">
      </div>
      <h2 class="title is-5">The Effectiveness of The Introduced Components</h2>
      <div class="content has-text-justified">
        <img style="margin-left: auto;margin-right: auto;display: block;" src="static/images/abl.png">
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Benchmark Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <table>
            <thead>
                <tr>
                    <th>Method</th>
                    <th style="width: 7em;">Extra Data</th>
                    <th>Description</th>
                    <th>MPJPE cm.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="width: 7em;">Zheng <i>et al.</i> <p style="color:darkgray; font-size: small;">CVPR 22</p></td>
                    <td>&#x2713;</td>
                    <td>Trained on 155,182 objects from internal data. Generated pseudo labels from 2D image labels.</td>
                    <td>10.80</td>
                </tr>
                <tr>
                    <td>GC-KPL <p style="color:darkgray; font-size: small;">CVPR 23</p></td>
                    <td>&#x2713;</td>
                    <td>Pre-trained on synthetic data. Fine-tuned on ground truth.</td>
                    <td>11.27</td>
                </tr>
                <tr>
                    <td></td>
                    <td>&#x2713;</td>
                    <td>Pre-trained on 200,000 Waymo objects. Fine-tuned on ground truth</td>
                    <td>10.10</td>
                </tr>
                <tr>
                    <td>HUM3DIL <p style="color:darkgray; font-size: small;">CoRL 22</p></td>
                    <td>&#x2717;</td>
                    <td>Randomly initialized</td>
                    <td>12.21</td>
                </tr>
                <tr>
                    <td>VoxelKP <p style="color:darkgray; font-size: small;">Ours</p></td>
                    <td>&#x2717;</td>
                    <td>Randomly initialized</td>
                    <td><b>8.87</b></td>
                </tr>
            </tbody>
        </table>
        <p><i>VoxelKP</i> achieved SOTA on Waymo dataset without using additional data.</p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
